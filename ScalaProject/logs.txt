SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tanay/.cache/coursier/v1/https/repo1.maven.org/maven2/ch/qos/logback/logback-classic/1.2.6/logback-classic-1.2.6.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tanay/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [ch.qos.logback.classic.util.ContextSelectorStaticBinder]
2022-04-16 16:10:13,459 WARN  org.apache.spark.util.Utils  - Your hostname, tanay-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
2022-04-16 16:10:13,462 WARN  org.apache.spark.util.Utils  - Set SPARK_LOCAL_IP if you need to bind to another address
2022-04-16 16:10:13,627 INFO  org.apache.spark.SparkContext  - Running Spark version 3.2.1
2022-04-16 16:10:13,817 WARN  o.a.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-04-16 16:10:13,920 INFO  o.a.spark.resource.ResourceUtils  - ==============================================================
2022-04-16 16:10:13,921 INFO  o.a.spark.resource.ResourceUtils  - No custom resources configured for spark.driver.
2022-04-16 16:10:13,921 INFO  o.a.spark.resource.ResourceUtils  - ==============================================================
2022-04-16 16:10:13,922 INFO  org.apache.spark.SparkContext  - Submitted application: SparkStreaming
2022-04-16 16:10:13,948 INFO  o.a.spark.resource.ResourceProfile  - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2022-04-16 16:10:13,961 INFO  o.a.spark.resource.ResourceProfile  - Limiting resource is cpu
2022-04-16 16:10:13,962 INFO  o.a.s.r.ResourceProfileManager  - Added ResourceProfile id: 0
2022-04-16 16:10:14,060 INFO  org.apache.spark.SecurityManager  - Changing view acls to: tanay
2022-04-16 16:10:14,061 INFO  org.apache.spark.SecurityManager  - Changing modify acls to: tanay
2022-04-16 16:10:14,061 INFO  org.apache.spark.SecurityManager  - Changing view acls groups to: 
2022-04-16 16:10:14,062 INFO  org.apache.spark.SecurityManager  - Changing modify acls groups to: 
2022-04-16 16:10:14,062 INFO  org.apache.spark.SecurityManager  - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tanay); groups with view permissions: Set(); users  with modify permissions: Set(tanay); groups with modify permissions: Set()
2022-04-16 16:10:14,369 INFO  org.apache.spark.util.Utils  - Successfully started service 'sparkDriver' on port 35705.
2022-04-16 16:10:14,405 INFO  org.apache.spark.SparkEnv  - Registering MapOutputTracker
2022-04-16 16:10:14,440 INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMaster
2022-04-16 16:10:14,461 INFO  o.a.s.s.BlockManagerMasterEndpoint  - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2022-04-16 16:10:14,462 INFO  o.a.s.s.BlockManagerMasterEndpoint  - BlockManagerMasterEndpoint up
2022-04-16 16:10:14,467 INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMasterHeartbeat
2022-04-16 16:10:14,494 INFO  o.a.spark.storage.DiskBlockManager  - Created local directory at /tmp/blockmgr-1ef686e0-e5f5-41bf-85a8-1ce25acc0262
2022-04-16 16:10:14,535 INFO  o.a.spark.storage.memory.MemoryStore  - MemoryStore started with capacity 880.5 MiB
2022-04-16 16:10:14,570 INFO  org.apache.spark.SparkEnv  - Registering OutputCommitCoordinator
2022-04-16 16:10:14,696 INFO  org.sparkproject.jetty.util.log  - Logging initialized @2883ms to org.sparkproject.jetty.util.log.Slf4jLog
2022-04-16 16:10:14,772 INFO  org.sparkproject.jetty.server.Server  - jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 1.8.0_322-b06
2022-04-16 16:10:14,808 INFO  org.sparkproject.jetty.server.Server  - Started @2996ms
2022-04-16 16:10:14,842 INFO  o.s.jetty.server.AbstractConnector  - Started ServerConnector@50b0bc4c{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-04-16 16:10:14,842 INFO  org.apache.spark.util.Utils  - Successfully started service 'SparkUI' on port 4040.
2022-04-16 16:10:14,871 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@71652c98{/jobs,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,873 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@3b4ef7{/jobs/json,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,874 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@5987e932{/jobs/job,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,877 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@25230246{/jobs/job/json,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,878 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@4a8b5227{/stages,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,879 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@6979efad{/stages/json,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,880 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@4a67318f{/stages/stage,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,883 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@54e81b21{/stages/stage/json,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,884 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@6650813a{/stages/pool,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,885 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@50cf5a23{/stages/pool/json,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,886 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@273c947f{/storage,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,886 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@1af1347d{/storage/json,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,887 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@20765ed5{/storage/rdd,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,889 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@2899a8db{/storage/rdd/json,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,889 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@c1a4620{/environment,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,893 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@130a0f66{/environment/json,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,894 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@12365c88{/executors,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,894 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@2237bada{/executors/json,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,895 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@5710768a{/executors/threadDump,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,896 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@6e0d4a8{/executors/threadDump/json,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,904 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@30272916{/static,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,905 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@3773862a{/,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,907 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@589b028e{/api,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,908 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@236ab296{/jobs/job/kill,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,909 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@63034ed1{/stages/stage/kill,null,AVAILABLE,@Spark}
2022-04-16 16:10:14,911 INFO  org.apache.spark.ui.SparkUI  - Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
2022-04-16 16:10:15,353 INFO  org.apache.spark.executor.Executor  - Starting executor ID driver on host 10.0.2.15
2022-04-16 16:10:15,412 INFO  org.apache.spark.util.Utils  - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33409.
2022-04-16 16:10:15,412 INFO  o.a.s.n.n.NettyBlockTransferService  - Server created on 10.0.2.15:33409
2022-04-16 16:10:15,425 INFO  o.apache.spark.storage.BlockManager  - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2022-04-16 16:10:15,432 INFO  o.a.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 10.0.2.15, 33409, None)
2022-04-16 16:10:15,438 INFO  o.a.s.s.BlockManagerMasterEndpoint  - Registering block manager 10.0.2.15:33409 with 880.5 MiB RAM, BlockManagerId(driver, 10.0.2.15, 33409, None)
2022-04-16 16:10:15,441 INFO  o.a.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 10.0.2.15, 33409, None)
2022-04-16 16:10:15,443 INFO  o.apache.spark.storage.BlockManager  - Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 33409, None)
2022-04-16 16:10:15,645 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@3afae281{/metrics/json,null,AVAILABLE,@Spark}
2022-04-16 16:10:15,923 INFO  o.a.spark.sql.internal.SharedState  - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2022-04-16 16:10:15,954 INFO  o.a.spark.sql.internal.SharedState  - Warehouse path is 'file:/home/tanay/projects/proj/CSYE7200FinalProject/ScalaProject/spark-warehouse'.
2022-04-16 16:10:15,967 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@2f1ea80d{/SQL,null,AVAILABLE,@Spark}
2022-04-16 16:10:15,968 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@f1a45f8{/SQL/json,null,AVAILABLE,@Spark}
2022-04-16 16:10:15,969 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@11bd803{/SQL/execution,null,AVAILABLE,@Spark}
2022-04-16 16:10:15,970 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@75798d03{/SQL/execution/json,null,AVAILABLE,@Spark}
2022-04-16 16:10:15,972 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@2e2f720{/static/sql,null,AVAILABLE,@Spark}
2022-04-16 16:10:18,975 INFO  o.a.s.s.e.s.s.StateStoreCoordinatorRef  - Registered StateStoreCoordinator endpoint
2022-04-16 16:10:18,995 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@7cd3e0da{/StreamingQuery,null,AVAILABLE,@Spark}
2022-04-16 16:10:18,996 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@67e77f52{/StreamingQuery/json,null,AVAILABLE,@Spark}
2022-04-16 16:10:18,997 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@5b14f482{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2022-04-16 16:10:18,999 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@1a785fd5{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2022-04-16 16:10:19,001 INFO  o.s.j.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@6b9697ae{/static/sql,null,AVAILABLE,@Spark}
2022-04-16 16:10:19,008 WARN  o.a.s.s.e.s.ResolveWriteToStream  - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e19f9878-0a0d-45da-a60a-412a0defbeb8. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-04-16 16:10:19,035 INFO  o.a.s.s.e.s.ResolveWriteToStream  - Checkpoint root /tmp/temporary-e19f9878-0a0d-45da-a60a-412a0defbeb8 resolved to file:/tmp/temporary-e19f9878-0a0d-45da-a60a-412a0defbeb8.
2022-04-16 16:10:19,036 WARN  o.a.s.s.e.s.ResolveWriteToStream  - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-04-16 16:10:19,173 INFO  o.a.s.s.e.s.CheckpointFileManager  - Writing atomically to file:/tmp/temporary-e19f9878-0a0d-45da-a60a-412a0defbeb8/metadata using temp file file:/tmp/temporary-e19f9878-0a0d-45da-a60a-412a0defbeb8/.metadata.cc2f9aae-74a9-4cd6-846b-1598f64e91aa.tmp
2022-04-16 16:10:19,365 INFO  o.a.s.s.e.s.CheckpointFileManager  - Renamed temp file file:/tmp/temporary-e19f9878-0a0d-45da-a60a-412a0defbeb8/.metadata.cc2f9aae-74a9-4cd6-846b-1598f64e91aa.tmp to file:/tmp/temporary-e19f9878-0a0d-45da-a60a-412a0defbeb8/metadata
2022-04-16 16:10:19,412 INFO  o.a.s.s.e.s.MicroBatchExecution  - Starting [id = a4a01701-fee9-4bc7-ba9c-1909926d3f78, runId = 6836d479-5ca2-4196-89db-1560d41de9b6]. Use file:/tmp/temporary-e19f9878-0a0d-45da-a60a-412a0defbeb8 to store the query checkpoint.
2022-04-16 16:10:19,438 INFO  o.a.s.s.e.s.MicroBatchExecution  - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@356ff880] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@5faa5b01]
2022-04-16 16:10:19,484 INFO  o.a.s.s.e.s.MicroBatchExecution  - Starting new streaming query.
2022-04-16 16:10:19,489 INFO  o.a.s.s.e.s.MicroBatchExecution  - Stream started from {}
2022-04-16 16:10:19,564 INFO  o.a.k.c.consumer.ConsumerConfig  - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-04-16 16:10:19,704 INFO  o.a.kafka.common.utils.AppInfoParser  - Kafka version: 2.8.0
2022-04-16 16:10:19,704 INFO  o.a.kafka.common.utils.AppInfoParser  - Kafka commitId: ebb1d6e21cc92130
2022-04-16 16:10:19,704 INFO  o.a.kafka.common.utils.AppInfoParser  - Kafka startTimeMs: 1650139819703
2022-04-16 16:10:19,708 INFO  o.a.k.clients.consumer.KafkaConsumer  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0] Subscribed to topic(s): songs-topic
2022-04-16 16:10:20,047 INFO  org.apache.kafka.clients.Metadata  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0] Cluster ID: 4hXqWDnTQq2Q7yBaHXALWg
2022-04-16 16:10:20,048 INFO  o.a.k.c.c.i.AbstractCoordinator  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0] Discovered group coordinator tanay-VirtualBox:9092 (id: 2147483647 rack: null)
2022-04-16 16:10:20,051 INFO  o.a.k.c.c.i.AbstractCoordinator  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0] (Re-)joining group
2022-04-16 16:10:20,064 INFO  o.a.k.c.c.i.AbstractCoordinator  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0] (Re-)joining group
2022-04-16 16:10:20,071 INFO  o.a.k.c.c.i.AbstractCoordinator  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0] Successfully joined group with generation Generation{generationId=1, memberId='consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1-f3ca5aa6-e9fb-4d81-90c3-2bfdbf26db26', protocol='range'}
2022-04-16 16:10:20,073 INFO  o.a.k.c.c.i.ConsumerCoordinator  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0] Finished assignment for group at generation 1: {consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1-f3ca5aa6-e9fb-4d81-90c3-2bfdbf26db26=Assignment(partitions=[songs-topic-0])}
2022-04-16 16:10:20,089 INFO  o.a.k.c.c.i.AbstractCoordinator  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0] Successfully synced group in generation Generation{generationId=1, memberId='consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1-f3ca5aa6-e9fb-4d81-90c3-2bfdbf26db26', protocol='range'}
2022-04-16 16:10:20,089 INFO  o.a.k.c.c.i.ConsumerCoordinator  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0] Notifying assignor about the new Assignment(partitions=[songs-topic-0])
2022-04-16 16:10:20,092 INFO  o.a.k.c.c.i.ConsumerCoordinator  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0] Adding newly assigned partitions: songs-topic-0
2022-04-16 16:10:20,163 INFO  o.a.k.c.c.i.ConsumerCoordinator  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0] Found no committed offset for partition songs-topic-0
2022-04-16 16:10:20,171 INFO  o.a.k.c.c.i.SubscriptionState  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0] Seeking to EARLIEST offset of partition songs-topic-0
2022-04-16 16:10:20,179 INFO  o.a.k.c.c.i.SubscriptionState  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0] Resetting offset for partition songs-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tanay-VirtualBox:9092 (id: 0 rack: null)], epoch=0}}.
2022-04-16 16:10:20,221 INFO  o.a.s.s.e.s.CheckpointFileManager  - Writing atomically to file:/tmp/temporary-e19f9878-0a0d-45da-a60a-412a0defbeb8/sources/0/0 using temp file file:/tmp/temporary-e19f9878-0a0d-45da-a60a-412a0defbeb8/sources/0/.0.5802d78a-e9d9-4f24-9a4b-bf62e1509b6e.tmp
2022-04-16 16:10:20,348 INFO  o.a.s.s.e.s.CheckpointFileManager  - Renamed temp file file:/tmp/temporary-e19f9878-0a0d-45da-a60a-412a0defbeb8/sources/0/.0.5802d78a-e9d9-4f24-9a4b-bf62e1509b6e.tmp to file:/tmp/temporary-e19f9878-0a0d-45da-a60a-412a0defbeb8/sources/0/0
2022-04-16 16:10:20,348 INFO  o.a.s.s.k.KafkaMicroBatchStream  - Initial offsets: {"songs-topic":{"0":0}}
2022-04-16 16:10:20,351 INFO  o.a.k.c.c.i.SubscriptionState  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0] Seeking to LATEST offset of partition songs-topic-0
2022-04-16 16:10:20,354 INFO  o.a.k.c.c.i.SubscriptionState  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0-1, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-driver-0] Resetting offset for partition songs-topic-0 to position FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tanay-VirtualBox:9092 (id: 0 rack: null)], epoch=0}}.
2022-04-16 16:10:20,401 INFO  o.a.s.s.e.s.CheckpointFileManager  - Writing atomically to file:/tmp/temporary-e19f9878-0a0d-45da-a60a-412a0defbeb8/offsets/0 using temp file file:/tmp/temporary-e19f9878-0a0d-45da-a60a-412a0defbeb8/offsets/.0.4ec0cb7e-788a-4b36-a8ae-785c07972ab8.tmp
2022-04-16 16:10:20,453 INFO  o.a.s.s.e.s.CheckpointFileManager  - Renamed temp file file:/tmp/temporary-e19f9878-0a0d-45da-a60a-412a0defbeb8/offsets/.0.4ec0cb7e-788a-4b36-a8ae-785c07972ab8.tmp to file:/tmp/temporary-e19f9878-0a0d-45da-a60a-412a0defbeb8/offsets/0
2022-04-16 16:10:20,457 INFO  o.a.s.s.e.s.MicroBatchExecution  - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1650139820367,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2022-04-16 16:10:20,966 INFO  o.a.s.s.k.KafkaOffsetReaderConsumer  - Partitions added: Map()
2022-04-16 16:10:21,129 INFO  o.a.s.s.k.KafkaOffsetReaderConsumer  - Partitions added: Map()
2022-04-16 16:10:21,218 INFO  o.a.s.s.k.KafkaOffsetReaderConsumer  - Partitions added: Map()
2022-04-16 16:10:21,220 INFO  o.a.s.s.k.KafkaOffsetReaderConsumer  - Partitions added: Map()
2022-04-16 16:10:21,753 INFO  o.a.s.s.c.e.codegen.CodeGenerator  - Code generated in 198.689659 ms
2022-04-16 16:10:22,001 INFO  o.a.s.s.c.e.codegen.CodeGenerator  - Code generated in 35.073438 ms
2022-04-16 16:10:22,118 INFO  org.apache.spark.SparkContext  - Starting job: start at SparkStreamingTweets.scala:48
2022-04-16 16:10:22,147 INFO  o.a.spark.scheduler.DAGScheduler  - Got job 0 (start at SparkStreamingTweets.scala:48) with 1 output partitions
2022-04-16 16:10:22,148 INFO  o.a.spark.scheduler.DAGScheduler  - Final stage: ResultStage 0 (start at SparkStreamingTweets.scala:48)
2022-04-16 16:10:22,149 INFO  o.a.spark.scheduler.DAGScheduler  - Parents of final stage: List()
2022-04-16 16:10:22,156 INFO  o.a.spark.scheduler.DAGScheduler  - Missing parents: List()
2022-04-16 16:10:22,175 INFO  o.a.spark.scheduler.DAGScheduler  - Submitting ResultStage 0 (MapPartitionsRDD[16] at start at SparkStreamingTweets.scala:48), which has no missing parents
2022-04-16 16:10:22,499 INFO  o.a.spark.storage.memory.MemoryStore  - Block broadcast_0 stored as values in memory (estimated size 35.4 KiB, free 880.5 MiB)
2022-04-16 16:10:22,538 INFO  o.a.spark.storage.memory.MemoryStore  - Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.7 KiB, free 880.5 MiB)
2022-04-16 16:10:22,541 INFO  o.a.spark.storage.BlockManagerInfo  - Added broadcast_0_piece0 in memory on 10.0.2.15:33409 (size: 15.7 KiB, free: 880.5 MiB)
2022-04-16 16:10:22,547 INFO  org.apache.spark.SparkContext  - Created broadcast 0 from broadcast at DAGScheduler.scala:1478
2022-04-16 16:10:22,559 INFO  o.a.spark.scheduler.DAGScheduler  - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[16] at start at SparkStreamingTweets.scala:48) (first 15 tasks are for partitions Vector(0))
2022-04-16 16:10:22,560 INFO  o.a.s.scheduler.TaskSchedulerImpl  - Adding task set 0.0 with 1 tasks resource profile 0
2022-04-16 16:10:22,635 INFO  o.a.spark.scheduler.TaskSetManager  - Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 5296 bytes) taskResourceAssignments Map()
2022-04-16 16:10:22,663 INFO  org.apache.spark.executor.Executor  - Running task 0.0 in stage 0.0 (TID 0)
2022-04-16 16:10:23,141 INFO  o.a.s.s.c.e.codegen.CodeGenerator  - Code generated in 23.885293 ms
2022-04-16 16:10:23,175 INFO  o.a.s.s.c.e.codegen.CodeGenerator  - Code generated in 21.314449 ms
2022-04-16 16:10:23,219 INFO  o.a.s.s.c.e.codegen.CodeGenerator  - Code generated in 14.39872 ms
2022-04-16 16:10:23,304 INFO  o.a.s.s.c.e.codegen.CodeGenerator  - Code generated in 77.679177 ms
2022-04-16 16:10:23,350 INFO  o.a.k.c.consumer.ConsumerConfig  - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-04-16 16:10:23,379 INFO  o.a.kafka.common.utils.AppInfoParser  - Kafka version: 2.8.0
2022-04-16 16:10:23,379 INFO  o.a.kafka.common.utils.AppInfoParser  - Kafka commitId: ebb1d6e21cc92130
2022-04-16 16:10:23,379 INFO  o.a.kafka.common.utils.AppInfoParser  - Kafka startTimeMs: 1650139823361
2022-04-16 16:10:23,384 INFO  o.a.k.clients.consumer.KafkaConsumer  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-executor-2, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-executor] Subscribed to partition(s): songs-topic-0
2022-04-16 16:10:23,418 INFO  o.a.k.clients.consumer.KafkaConsumer  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-executor-2, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-executor] Seeking to offset 0 for partition songs-topic-0
2022-04-16 16:10:23,435 INFO  org.apache.kafka.clients.Metadata  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-executor-2, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-executor] Cluster ID: 4hXqWDnTQq2Q7yBaHXALWg
2022-04-16 16:10:23,519 INFO  o.a.k.c.c.i.SubscriptionState  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-executor-2, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-executor] Seeking to EARLIEST offset of partition songs-topic-0
2022-04-16 16:10:24,020 INFO  o.a.k.c.c.i.SubscriptionState  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-executor-2, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-executor] Resetting offset for partition songs-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tanay-VirtualBox:9092 (id: 0 rack: null)], epoch=0}}.
2022-04-16 16:10:24,020 INFO  o.a.k.c.c.i.SubscriptionState  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-executor-2, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-executor] Seeking to LATEST offset of partition songs-topic-0
2022-04-16 16:10:24,021 INFO  o.a.k.c.c.i.SubscriptionState  - [Consumer clientId=consumer-spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-executor-2, groupId=spark-kafka-source-9c2be6be-7354-4603-8c7b-c02fa0306023--619656683-executor] Resetting offset for partition songs-topic-0 to position FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tanay-VirtualBox:9092 (id: 0 rack: null)], epoch=0}}.
2022-04-16 16:10:24,323 INFO  o.a.s.s.c.e.codegen.CodeGenerator  - Code generated in 11.465961 ms
2022-04-16 16:10:24,406 INFO  o.a.spark.storage.memory.MemoryStore  - Block rdd_10_0 stored as values in memory (estimated size 33.4 KiB, free 880.4 MiB)
2022-04-16 16:10:24,407 INFO  o.a.spark.storage.BlockManagerInfo  - Added rdd_10_0 in memory on 10.0.2.15:33409 (size: 33.4 KiB, free: 880.5 MiB)
2022-04-16 16:10:24,418 INFO  o.a.s.s.c.e.codegen.CodeGenerator  - Code generated in 5.17107 ms
2022-04-16 16:10:24,454 INFO  o.a.s.s.c.e.codegen.CodeGenerator  - Code generated in 27.772386 ms
2022-04-16 16:10:24,667 INFO  e.s.nlp.pipeline.StanfordCoreNLP  - Adding annotator tokenize
2022-04-16 16:10:24,673 INFO  e.s.nlp.pipeline.TokenizerAnnotator  - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
2022-04-16 16:10:24,687 INFO  e.s.nlp.pipeline.StanfordCoreNLP  - Adding annotator ssplit
2022-04-16 16:10:24,699 INFO  e.s.nlp.pipeline.StanfordCoreNLP  - Adding annotator parse
2022-04-16 16:10:24,737 INFO  e.s.nlp.parser.common.ParserGrammar  - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.5 sec].
2022-04-16 16:10:25,207 INFO  e.s.nlp.pipeline.StanfordCoreNLP  - Adding annotator sentiment
-1
